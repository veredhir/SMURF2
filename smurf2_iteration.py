import globimport osimport os.pathfrom __builtin__ import staticmethodfrom ast import literal_evalfrom itertools import izipimport mathimport numpy as npimport pandas as pdfrom Bio import SeqIOfrom seqBIn import *from multiprocessing import Pool, cpu_countimport itertoolsimport psutilfrom functools import partialimport traceback# import dask.dataframe as dd# I tried to use dask for time optimization. it didn't work well, code in gitfrom smurf2_headers import *from smurf2_const import qualsfrom smurf2_utills import *class Primer(object):    """    initialize and store the primers for each amplified region    Require initialization using primer.csv file        The header contain the amplified regions        Each row in the table contain single primer used to amplify the corresponding region.    """    def __init__(self):        self.all = {}        self.histogram = {}        self.min_size = 100        self.similarity_score_th = 0.7    def init(self, primers_path):        """        Init the primers using the primers.csv file        primers_path: string                      path to primers.csv file:                      The header contain the amplified regions                      Each row in the table contain single primer used to amplify the corresponding region.        """        primers_df = pd.read_csv(primers_path, index_col=False)        for region in primers_df.columns:            self.add_primer_for_region(int(region), primers_df[region].fillna('').tolist())        logging.info(            "Primers were update with {} regions, min primer size = {}".format(len(primers_df.columns), self.min_size))    def add_primer_for_region(self, region, primers_list):        """        Add list of primers for region        Initiate the primers histogram        region: int                The considered region        primers_list: list of strings                The list of string corresponding to the region        """        primers_list = filter(lambda x: x != '', primers_list)        self.all.update({region: primers_list})        for primer in primers_list:            self.histogram.update({primer: 0})            if len(primer) < self.min_size:                self.min_size = len(primer)    def print_histogram(self):        logging.debug("=============================")        logging.debug("PRIMERS HISTOGRAM:")        for primer, counter in self.histogram.iteritems():            logging.debug("{} --> {}".format(primer, counter))        logging.debug("=============================")    def update_primers_using_histogram(self, limit=20):        """        Detect which primers are in use, and which could be neglect        The primers histogram reflect how many reads where amplified using each primer        limit: int               The minimum histogram value for one primer to consider "in use" primer               Primers with value below limit will be removed        """        self.print_histogram()        for i, primers in self.all.iteritems():            new_primers = []            for primer in primers:                if self.histogram[primer] > limit:                    new_primers.append(primer)            logging.info(                "Region {}: # of new primers = {}, # of old primers = {}".format(i, len(new_primers), len(primers)))            self.all[i] = new_primers    def get_region_for_read(self, read):        """        Find the region for a given read        Go over all the primers, and find the primer with the best match.        If the best match similarity score is lower than minimum_match_score, read is invalid.        read: string        return: int or None                The region match for the given read, in case of invalid similarity score, return None        """        match_size = self.min_size        read_primer = read[:match_size]        best_score = self.similarity_score_th        region = None        best_primer = None        for i, primers in self.all.iteritems():            similarity_scores = [similar(read_primer, primer[:match_size]) for primer in primers]            score = max(similarity_scores)            if score >= best_score:                best_score = score                best_primer = primers[np.argmax(similarity_scores)]                region = i        # update the primers histogram from the reads:        if region is not None:            self.histogram[best_primer] += 1        return regionclass Smurf2Thresholds:    """Hold the thresholds for smurf2 computations    --Split thresholds--    max_changed_bases_rate_for_split: float                                      Limit the amount of bases change in one split to this threshold, the split                                      sequence can't be too different from the original sequence    min_coverage_for_split: float                            The split candidate must have estimated coverage >= this threshold    min_minor_prob_for_split: float                              minimum probability of second most probable base at a site                              required in order to call site a variant    --merge similar reference sequences-- min_similar_bases_rate_for_merge: float If two candidate sequences share >=    this fractional identity over their bases with mapped reads, then merge the two sequences into one for the next    iteration.    --stability-- max_priors_diff_for_stability_test: float In stable state the difference between the previous    iteration priors and the current priors " should be less then this threshold    --initialization--    min_initial_frequency: float                           In each iteration, filter the reference with frequency below this threshold    """    def __init__(self,                 max_changed_bases_rate_for_split,                 min_coverage_for_split,                 min_minor_prob_for_split,                 min_similar_bases_rate_for_merge,                 max_priors_diff_for_stability_test,                 max_distance_from_database=15,                 n_regions=5):        self.max_changed_bases_rate_for_split = max_changed_bases_rate_for_split        self.min_coverage_for_split = min_coverage_for_split        self.min_minor_prob_for_split = min_minor_prob_for_split        self.min_similar_bases_rate_for_merge = min_similar_bases_rate_for_merge        self.max_priors_diff_for_stability_test = max_priors_diff_for_stability_test        self.max_distance_from_database = max_distance_from_database        self.min_initial_frequency = 0.1 * min(self.min_coverage_for_split, max_priors_diff_for_stability_test)        self.min_freq_of_mapped_reads_for_region = self.min_initial_frequency / float(n_regions)        logging.info("min_coverage_for_split = {}, max_priors_diff_for_stability_test = {}, min_initial_frequency = {}"                     .format(self.min_coverage_for_split,                             self.max_priors_diff_for_stability_test,                             self.min_initial_frequency))class Smurf2Paths:    """    Holds smurf2 files paths    """    def __init__(self, working_dir):        self.reference = os.path.join(working_dir, "reference_db.csv")        self.full_reference = ""        self.current_state = os.path.join(working_dir, "curr_state.csv")        self.final_results = os.path.join(working_dir, "final_results.csv")        self.mapping = os.path.join(working_dir, "mapping.csv")        self.unique_ref_to_ref = os.path.join(working_dir, "unique_ref_id_to_ref_id.csv")        self.posteriors = os.path.join(working_dir, "posteriors.csv")        self.read_quals = os.path.join(working_dir, "reads_db.csv")        self.prob_n = {Base.A: os.path.join(working_dir, "prob_A.csv"),                       Base.C: os.path.join(working_dir, "prob_C.csv"),                       Base.G: os.path.join(working_dir, "prob_G.csv"),                       Base.T: os.path.join(working_dir, "prob_T.csv")}        self.tmp_dir = working_dir        self.prev_prob_n = {}    def update_full_reference_path(self, fasta_path, read_len):        """        Full reference is saved with the fasta files        It produce once for each read length, and could be used again        """        self.full_reference = os.path.join(fasta_path, "full_reference_db_{}.csv".format(read_len))def _get_new_references(test, reference_suffix):    # FIND BEST MATCH REFERENCES    full_probs_df = test.copy()    best_match = full_probs_df.copy()    best_binary_sequence = full_probs_df['best_sequence'].apply(sequence_to_bin)    best_match[Base.A] = best_binary_sequence.apply(lambda r: r[0])    best_match[Base.C] = best_binary_sequence.apply(lambda r: r[1])    best_match[Base.G] = best_binary_sequence.apply(lambda r: r[2])    best_match[Base.T] = best_binary_sequence.apply(lambda r: r[3])    # if split -> change the priors to prior*(1-avg_minor) else prior stay the same.    best_match[CurrentStateFormat.Priors].update(        best_match[CurrentStateFormat.Priors] * (1 - best_match['is_split'] * best_match['avg_minor']))    # FIND SECOND BEST REFERENCES    second_best_match = full_probs_df[full_probs_df['is_split'] == True]    if len(second_best_match) > 0:        logging.info('************************')        logging.info("splitting {} references".format(            len(second_best_match.drop_duplicates(CurrentStateFormat.Reference_id))))        logging.info(            "splitting {}".format(second_best_match[CurrentStateFormat.Reference_id].drop_duplicates().tolist()))        logging.info('************************\n')        second_best_match[CurrentStateFormat.Reference_id].update(            second_best_match[CurrentStateFormat.Reference_id].apply(lambda r: r + reference_suffix))        second_best_binary_sequence = second_best_match['2-best_sequence'].apply(sequence_to_bin)        second_best_match.loc[:, Base.A] = second_best_binary_sequence.apply(lambda r: r[0])        second_best_match.loc[:, Base.C] = second_best_binary_sequence.apply(lambda r: r[1])        second_best_match.loc[:, Base.G] = second_best_binary_sequence.apply(lambda r: r[2])        second_best_match.loc[:, Base.T] = second_best_binary_sequence.apply(lambda r: r[3])        second_best_match[CurrentStateFormat.Priors].update(            second_best_match[CurrentStateFormat.Priors] * second_best_match['avg_minor'])    new_refs = pd.concat([best_match, second_best_match], ignore_index=True)[CurrentStateFormat.Bases.all +                                                                             [CurrentStateFormat.Reference_id,                                                                              CurrentStateFormat.Priors,                                                                              CurrentStateFormat.Region,                                                                              CurrentStateFormat.Weight]]    logging.info("References: {}".format(new_refs[CurrentStateFormat.Reference_id].drop_duplicates().tolist()))    return new_refsdef _get_new_posterior(max_prob_n_full_data, full_posteriors, reference_suffix):    reference_minors = max_prob_n_full_data[        ['is_split', 'avg_minor', CurrentStateFormat.Reference_id]].drop_duplicates()    # we need only reference from the original posteriors dataframe    posteriors = reference_minors.merge(full_posteriors, on=CurrentStateFormat.Reference_id, how='right')    posteriors[PosteriorsFormat.Posterior] = posteriors[PosteriorsFormat.Posterior] * (            1 - posteriors['is_split'] * posteriors['avg_minor'])    # Create the new posteriors (of the splitted references)    new_posteriors = posteriors[posteriors['is_split'] == True]    new_posteriors[CurrentStateFormat.Reference_id].update(        new_posteriors[CurrentStateFormat.Reference_id].apply(lambda r: r + reference_suffix))    new_posteriors[PosteriorsFormat.Posterior].update(        new_posteriors[PosteriorsFormat.Posterior] * new_posteriors['avg_minor'])    posteriors_df = pd.concat([posteriors, new_posteriors], ignore_index=True)[[PosteriorsFormat.Posterior,                                                                                PosteriorsFormat.Read_id,                                                                                PosteriorsFormat.Ref_id]]    logging.debug(        "Posteriors - References: {}".format(posteriors_df[PosteriorsFormat.Ref_id].drop_duplicates().tolist()))    return posteriors_df.copy()def _get_new_posteriors_and_priors(posteriors_df, minors, refs):    if len(minors) > 0:        minor_fraction_avg = np.mean(minors)    else:        minor_fraction_avg = 0    ref_ids = list(set([str(r.get(CurrentStateFormat.Reference_id)) for r in refs]))    ref_ids = sorted(ref_ids, key=len, reverse=False)    if len(ref_ids) > 1:        for ref in refs:            if str(ref.get(CurrentStateFormat.Reference_id)) == ref_ids[0]:                ref[CurrentStateFormat.Priors] = ref[CurrentStateFormat.Priors] * (1 - minor_fraction_avg)            else:                ref[CurrentStateFormat.Priors] = ref[CurrentStateFormat.Priors] * minor_fraction_avg    new_posteriors = []    for _, posterior in posteriors_df.iterrows():        posterior_dict = {PosteriorsFormat.Ref_id: ref_ids[0],                          PosteriorsFormat.Read_id: posterior[PosteriorsFormat.Read_id],                          PosteriorsFormat.Posterior: posterior[PosteriorsFormat.Posterior] * (                                  1 - minor_fraction_avg)}        new_posteriors += [posterior_dict]        if minor_fraction_avg and len(ref_ids) > 1:            posterior_dict = {PosteriorsFormat.Ref_id: ref_ids[1],                              PosteriorsFormat.Read_id: posterior[PosteriorsFormat.Read_id],                              PosteriorsFormat.Posterior: posterior[PosteriorsFormat.Posterior] * (                                  minor_fraction_avg)}            new_posteriors += [posterior_dict]    return new_posteriors, refsclass Smurf2Iteration(object):    """    Hold all the data needed for smurf2 single iteration    and all the methods needed to calculate single iteration output.    """    def __init__(self, working_dir,                 prev_smurf2_iteration=None,                 fastq_path=None,                 reversed_fastq_path=None,                 primers_path=None,                 fasta_path=None,                 read_len=None,                 n_regions=5,                 update_weight_using_the_reads=False,                 # thresholds:                 max_changed_bases_rate_for_split=0.0025,  # (2.5%)                 min_coverage_for_split=0.005,  # (0.5%)                 min_minor_prob_for_split=0.08,                 min_similar_bases_rate_for_merge=0.999,  # (99%)                 max_priors_diff_for_stability_test=0.005,                 allow_split=False,                 debug_mode=True):        """        :param working_dir: string                            Current iteration data directory        :param prev_smurf2_iteration: Smurf2Iteration object                                      Previous iteration data        :param fastq_path: string                           Path to the reads fastq file        :param reversed_fastq_path: string                                    Path to the reversed reads fastq file        :param primers_path: string                             Path to csv file containing table with the regions as header,                             each column contains the primers for the specific region.        :param fasta_path: string                           Path to directory containing the fasta files for each region        :param read_len: int                         Read length        :param n_regions: int                         Amount of amplified regions        :param update_weight_using_the_reads: bool                                              Update the reference db with the number                                              of the region that were amplified according to the reads.        :param max_changed_bases_rate_for_split: float                                                 Limit the amount of bases change in one split to this threshold, the split                                                 sequence can't be too different from the original sequence        :param min_coverage_for_split: float                                       The split candidate must have estimated coverage >= this threshold        :param min_minor_prob_for_split: float                                         minimum probability of second most probable base at a site                                         required in order to call site a variant        :param min_similar_bases_rate_for_merge: float                                                 If two candidate sequences share >= this fractional identity over their bases with mapped reads,                                                 then merge the two sequences into one for the next iteration.        :param max_priors_diff_for_stability_test: float                                                   In stable state the difference between the previous iteration priors and the current priors "                                                   should be less then this threshold        :param allow_split: bool        :debug_mode: bool        """        self.debug_mode = False        self.iteration_index = 0        self.read_len = 0        self.allow_split = False        self.n_regions = n_regions        self.primers = Primer()        self.paths = Smurf2Paths(working_dir)        self.th = Smurf2Thresholds(max_changed_bases_rate_for_split,                                   min_coverage_for_split,                                   min_minor_prob_for_split,                                   min_similar_bases_rate_for_merge,                                   max_priors_diff_for_stability_test)        self.ref_format = ReferenceFormat()        self.reads_full_data_format = ReadsFullDataFormat()        self.prev_priors_for_stability_test = None        self.n_reads = 0        # First EMIRGE iteration:        if prev_smurf2_iteration is None:            self.init_pre_process(primers_path,                                  fastq_path,                                  reversed_fastq_path,                                  read_len,                                  fasta_path,                                  update_weight_using_the_reads,                                  allow_split,                                  debug_mode)        else:            self.init(prev_smurf2_iteration)    @time_it    def init_pre_process(self,                         primers_path,                         fastq_path,                         reversed_fastq_path,                         read_len,                         fasta_path,                         update_weight_using_the_reads,                         allow_split,                         debug_mode):        """        Initialize the first smurf2 iteration        1. initiate parameters        2. Initiate the primers        3. classify reads into regions        4. Prepare the references        5. Find initial mapping        6. Find initial weight (How many region in the original reference)        7. Find initial priors        """        logging.info("ITERATION {}".format(self.iteration_index))        logging.info("number of regions = {}".format(self.n_regions))        self.read_len = read_len        self.n_reads = 0        self.paths.update_full_reference_path(fasta_path, self.read_len)        self.debug_mode = debug_mode        self.iteration_index = 0        self.allow_split = allow_split        self.primers.init(primers_path)        reads_df = self.prepare_reads(fastq_path, reversed_fastq_path, read_len)        # After classifying all the reads into regions using the primers        # update the primers used        self.primers.update_primers_using_histogram()        # Process the reference        # The reference database is given as fasta files.        # SMURF2 uses csv format.        if not self.is_processed_reference_file_exists():            self.prepare_references(fasta_path)        self.get_unique_amplified_references()        self._find_mapping(reads_df)        self._find_initial_weight(update_weight_using_the_reads)        self.calc_initial_priors()    @time_it    def init(self, prev_iteration):        """        Initialize class using the previous iteration.        Copy data from previous iteration.        Find the mapping for current iteration        (The references are different since they were change in the end of the previous iteration)        :param prev_iteration: type EmirgeIteration        """        self.iteration_index = prev_iteration.iteration_index + 1        logging.info("ITERATION {}".format(self.iteration_index))        self.debug_mode = prev_iteration.debug_mode        self.n_regions = prev_iteration.n_regions        self.allow_split = prev_iteration.allow_split        self.read_len = prev_iteration.read_len        self.paths.read_quals = prev_iteration.paths.read_quals        self.paths.prev_prob_n = prev_iteration.paths.prob_n        self.paths.unique_ref_to_ref = prev_iteration.paths.unique_ref_to_ref        self.th = prev_iteration.th        self.n_reads = prev_iteration.n_reads        curr_state_df = pd.read_csv(prev_iteration.paths.current_state, index_col=False)        curr_state_df = curr_state_df[[CurrentStateFormat.Reference_id,                                       CurrentStateFormat.Region,                                       CurrentStateFormat.Weight,                                       CurrentStateFormat.Priors] +                                      CurrentStateFormat.Bases.all]        curr_state_df = curr_state_df.drop_duplicates()        curr_state_df.to_csv(self.paths.current_state, index=False)        self.prev_priors_for_stability_test = curr_state_df[[CurrentStateFormat.Reference_id,                                                             CurrentStateFormat.Priors]].drop_duplicates()        # Keep only references with significant frequency        reference_df = curr_state_df[curr_state_df[CurrentStateFormat.Priors] > self.th.min_initial_frequency]        logging.info("Current state # references before filter = {}, after filter above threshold [{}] # = {}"                     .format(len(self.prev_priors_for_stability_test),                             self.th.min_initial_frequency,                             len(reference_df[                                     [CurrentStateFormat.Reference_id, CurrentStateFormat.Priors]].drop_duplicates())))        reference_df = reference_df[[CurrentStateFormat.Reference_id,                                     CurrentStateFormat.Region,                                     CurrentStateFormat.Weight] +                                    Base.all].drop_duplicates()        reference_df.to_csv(self.paths.reference, index=False)        posteriors_df = pd.read_csv(prev_iteration.paths.posteriors, index_col=False)        posteriors_df.to_csv(self.paths.posteriors, index=False)        reads_df = pd.read_csv(prev_iteration.paths.mapping,                               index_col=False)[[MappingForamt.Region,                                                 MappingForamt.Group_id,                                                 MappingForamt.Count] +                                                Base.all].drop_duplicates()        self._find_mapping(reads_df)    def is_processed_reference_file_exists(self):        """        Return true if the reference in smurf2 csv format exists, otherwise return false        """        if os.path.exists(self.paths.full_reference):            return True        else:            logging.info("full reference file {} do not exists".format(self.paths.full_reference))            return False    @time_it    def process_reference_in_region(self, fasta_path, read_length, region_ix):        """        Go over the fasta file contains all the reference sequences in one region.        Extract the relevant sequence for each reference (support paired end, 2*read_length)        Return dataframe: columns: Reference id, region, A, C, G, T in binary format.        """        logging.info("Start processing fasta, path = %s", fasta_path)        data_dicts = []        for record in SeqIO.parse(fasta_path, "fasta"):            # logging.debug("Start processing fasta, path = %s", fasta_path)            sequence = record.seq.__str__()            title = record.id.__str__()            record_dict = self.ref_format.get_ref_dict(title, region_ix, sequence_to_bin(                sequence[0:read_length] + sequence[-1 * read_length:]))            data_dicts.append(record_dict)        logging.debug("Created dict for region = {}".                      format(region_ix))        ref_df = pd.DataFrame(data_dicts, columns=ReferenceFormat.temp_all)        return ref_df    @staticmethod    def extract_region_from_fasta_name(fasta_file_name, n_regions):        """        Extact the region index from fasta name        Region index should be include in the fasta file name.        Return int: region index        """        for i in range(1, n_regions + 1):            if str(i) in fasta_file_name:                return i        raise Exception("Invalid file name: {}, doesn't contain region id".format(n_regions))    @time_it    def get_mapped_references(self):        """        Read the mapping and reference dataframes and extract only the mapped references.        """        mapping_df = pd.read_csv(self.paths.mapping, index_col=False)        full_ref_df = pd.read_csv(self.paths.reference, index_col=False)        # Merging - taking only mapped references in mapped regions        # e.g after conversation with Noam and Gary at 14-03-2017, do not assume all regions were sampled correctly.        # TODO: Review it: I drop all reference in region without mapping.        mapped_reference = mapping_df[[MappingForamt.Ref_id, MappingForamt.Region]].drop_duplicates()        curr_ref_df = mapped_reference.merge(full_ref_df,                                             on=[HeadersFormat.Unique_Ref_id, HeadersFormat.Region],                                             how='left')        logging.info("Mapped references: {}/{}".format(            len(curr_ref_df.drop_duplicates(CurrentStateFormat.Reference_id)),            len(full_ref_df.drop_duplicates(CurrentStateFormat.Reference_id))))        return curr_ref_df    @time_it    def calc_initial_priors(self):        """        calc prior for each reference, e.g: P(Si) for each reference Si.        P(Si) = (sum_j(P(rj|Si)/Wi))/ sum_k(sum_j(P(rj|Sk)/Wk)            where P(rj|Si) is equal to 1 if the read j mapped to Si and zero otherwise.            if, for example, rj is mapped with the same probability to Si and Sj then, P(ri|Si) = P(ri|Sk) = 0.5.        1. calculate P(rj|Si)        2. sum_j(P(rj|Si))        3. sum_j(P(rj|Si)/Wi)        4. sum_k(sum_j(P(rj|Sk)/Wk)        5. (sum_j(P(rj|Si)/Wi))/ sum_k(sum_j(P(rj|Sk)/Wk)        """        mapping_df = pd.read_csv(self.paths.mapping, index_col=False)        # get all the mapped reference and all their data:        mapped_references = self.get_mapped_references()        # 1. calculate P(rj|Si): Calculate how many reads were mapped to single reference        # For each reads group - reference map: calculate how many reads were map to single reference.        # Since one read can be mapped to multiple reference: [amount of reads in the group]/[amount of reference this group map to]        mapping_df['mapped_reads_counter'] = mapping_df[MappingForamt.Count] * mapping_df[MappingForamt.Map_weight]        # 2. sum_j(P(rj|Si))        # Sum how many weighted reads mapped to each reference (sum over different reads in different regions)        ref_with_counter_df = mapping_df.groupby(MappingForamt.Ref_id)['mapped_reads_counter'].sum().reset_index()        # Add the reference weight to each reference        curr_state_with_priors = ref_with_counter_df.merge(mapped_references, on=CurrentStateFormat.Reference_id,                                                           how="left")        # 3. sum_j(P(rj|Si)/Wi)        curr_state_with_priors[CurrentStateFormat.Priors] = curr_state_with_priors['mapped_reads_counter'].astype(            float) / curr_state_with_priors[CurrentStateFormat.Weight].astype(float)        # 4. sum_k(sum_j(P(rj|Sk)/Wk)        # sum over the priors (on the reference without the regions duplication.)        priors_weight = \            curr_state_with_priors[[CurrentStateFormat.Reference_id, CurrentStateFormat.Priors]].drop_duplicates()[                CurrentStateFormat.Priors].sum()        if len(curr_state_with_priors[CurrentStateFormat.Reference_id].unique()) != len(                curr_state_with_priors[[CurrentStateFormat.Reference_id, CurrentStateFormat.Priors]].drop_duplicates()):            logging.error("There is a bug in here!!!!! prior should be unique per reference")        # 5. (sum_j(P(rj|Si)/Wi))/ sum_k(sum_j(P(rj|Sk)/Wk): normalize by the priors overall weight.        curr_state_with_priors[CurrentStateFormat.Priors] = curr_state_with_priors[                                                                CurrentStateFormat.Priors] / priors_weight        curr_state_columns = [CurrentStateFormat.Reference_id, CurrentStateFormat.Region, CurrentStateFormat.Weight,                              CurrentStateFormat.Priors] + CurrentStateFormat.Bases.all        curr_state_with_priors = curr_state_with_priors[curr_state_columns]        curr_state_with_priors = curr_state_with_priors.drop_duplicates()        curr_state_with_priors.to_csv(self.paths.current_state)    @time_it    def _find_initial_weight(self, use_weight_using_reads):        """        In the first iteration, get only reference which mapped to all existing regions of the reference.        :param reads_df:        :return:        """        full_ref_df = pd.read_csv(self.paths.reference, index_col=False)        mapping_df = pd.read_csv(self.paths.mapping, index_col=False)        all_mapped_refs = mapping_df[[MappingForamt.Ref_id, MappingForamt.Region]].drop_duplicates()        if use_weight_using_reads:            ref_with_weight = all_mapped_refs.groupby([MappingForamt.Ref_id]).count().reset_index()        else:            ref_with_weight = full_ref_df.groupby([MappingForamt.Ref_id]).count().reset_index()        ref_with_weight.rename(columns={MappingForamt.Region: CurrentStateFormat.Weight}, inplace=True)        ref_with_weight = ref_with_weight.drop_duplicates()[            [CurrentStateFormat.Weight, CurrentStateFormat.Reference_id]]        refs_without_weight = full_ref_df[ReferenceFormat.Bases.all + [ReferenceFormat.Ref_Id,                                                                       ReferenceFormat.Region]]        # get the actual weight for each mapped reference        ref_with_weight = pd.merge(refs_without_weight,                                   ref_with_weight,                                   on=ReferenceFormat.Ref_Id,                                   how='right')        ref_with_weight.to_csv(self.paths.reference, index=False)    @time_it    def _find_mapping(self, unique_reads_df, full_ref_df=None):        if full_ref_df is None:            full_ref_df = pd.read_csv(self.paths.reference, index_col=False)        ref_df_copy = full_ref_df.copy()        for base in Base.all:            full_ref_df[base].update(full_ref_df[base].apply(lambda r: int(r)))            unique_reads_df[base] = unique_reads_df[base].apply(lambda r: int(r))        unique_reads_for_region_groups = unique_reads_df.groupby(HeadersFormat.Region)        references_grouped_by_regions = full_ref_df.groupby(HeadersFormat.Region)        rename_base_dict = {}        for base in Base.all:            rename_base_dict.update({base + '_y': base})  # keep only the reads bases            rename_base_dict.update({base + '_y': base})  # keep only the reads bases        mapping_dfs = []        for region, reads_df in unique_reads_for_region_groups:            reads_df.reset_index(inplace=True)            try:                ref_df = references_grouped_by_regions.get_group(region)            except:                logging.warn("Skip region {}, no reference match to the region.".format(region))                continue            ref_df.reset_index(inplace=True)            # For time optimization: use unique sequences, and later merge back the additional reference data.            ref_df_unique = ref_df.drop_duplicates(Base.all)            # ref_df_unique = ref_df_unique[:1000] #TODO: FOR TEST: REMOVE!!!!            logging.info("before = {} after= {}".format(len(ref_df), len(ref_df_unique)))            ref_df_size = ref_df_unique.memory_usage(index=True, deep=True).sum()            row_size_ref_df = ref_df_size / len(ref_df_unique)            reads_df_size = reads_df.memory_usage(index=True, deep=True).sum()            row_size_reads_df = reads_df_size / len(reads_df)            merged_row_size = row_size_reads_df + row_size_ref_df            merge_size = merged_row_size * len(reads_df) * len(ref_df_unique) / float(10 ** 9)            free_memory = psutil.virtual_memory().free / float(10 ** 9)            n_cpu = cpu_count()            n_chunk = int(5 * int(merge_size / float(free_memory) + 1))            logging.info(                'Mapping for region = {}, reads size = {}, ref size = {},  amount of chunks = {}, cores={}, '                'free memory = {}G'.format(region, len(reads_df), len(ref_df_unique), n_chunk, n_cpu, free_memory))            # Find the number of matches between each read - ref couple:            min_score_for_mapping = 2 * self.read_len - self.th.max_distance_from_database            reads_and_refs_df = parallelize_mapping(reads_df, ref_df_unique, 2 * self.read_len, n_chunk,                                                    min_score_for_mapping)            logging.info("Done filtering")            cols = reads_and_refs_df.columns            # take the unique_reference_id from the ref_id dataframe            reads_and_refs_df = reads_and_refs_df.merge(ref_df, left_on=['A_ref', 'C_ref', 'G_ref', 'T_ref'],                                                        right_on=['A', 'C', 'G', 'T'], suffixes=('_tmp', ''),                                                        how='left')            reads_and_refs_df = reads_and_refs_df[cols]            logging.info("Done Merge after filtering")            rename_base_dict = {}            for base in Base.all:                rename_base_dict.update({base + '_read': base})  # keep only the reads bases            reads_and_refs_df.rename(columns=rename_base_dict, inplace=True)            reads_and_refs_df[MappingForamt.Map_weight] = 0            logging.debug(                "old columns = {}, wanted columns = {}".format(reads_and_refs_df.columns, MappingForamt.full_header))            reads_and_refs_df = reads_and_refs_df[MappingForamt.full_header]            mapping_dfs.append(reads_and_refs_df)            logging.info("Found mapping for region = {}".format(region))        mapping_df = pd.concat(mapping_dfs)        mapping_df = self.merge_similar_mapped_reference(mapping_df.copy(), ref_df_copy)        # Find to how many refs each read group was mapped(approximate)        mapping_df[MappingForamt.Map_weight] = 1.0 / mapping_df.groupby(HeadersFormat.Group_id)[            MappingForamt.Count].transform('count')        if abs((sum(unique_reads_df.Count) - int(sum(mapping_df.Count * mapping_df.Map_weight)))) > 20:            # raise Exception(            #     "MAPPING ERROR: the count is wrong! unique_reads = {}, counter = {}".format(sum(unique_reads_df.Count),            #                                                                                 sum(mapping_df.Count * mapping_df.Map_weight)))            logging.info("MAPPING WARNING: dropped reads due to non sufficient mapping before {};  after = {}"                         .format(sum(unique_reads_df.Count),                                 sum(mapping_df.Count * mapping_df.Map_weight)))        # Filter regions with low frequency        mapping_columns = mapping_df.columns        min_mapped_reads_for_region = max(int(self.n_reads * self.th.min_freq_of_mapped_reads_for_region), 5)        logging.info(            'min_mapped_reads_for_region = {}, mapping size before filter = {}'.format(min_mapped_reads_for_region,                                                                                       len(mapping_df)))        mapping_df['weighted_count'] = mapping_df[MappingForamt.Count] * mapping_df[MappingForamt.Map_weight]        mapping_df['region_in_ref_size'] = mapping_df.groupby([MappingForamt.Ref_id, MappingForamt.Ref_id])[            'weighted_count'].transform('sum')        mapping_df = mapping_df[mapping_df['region_in_ref_size'] > min_mapped_reads_for_region]        logging.info(            'min_mapped_reads_for_region = {}, mapping size after filter = {}'.format(min_mapped_reads_for_region,                                                                                      len(mapping_df)))        # Find to how many refs each read group was mapped(approximate) - update after filtering        mapping_df[MappingForamt.Map_weight] = 1.0 / mapping_df.groupby(HeadersFormat.Group_id)[            MappingForamt.Count].transform('count')        mapping_df = mapping_df[mapping_columns]        # Filter reference to contain only the relevant references.        mapped_references = mapping_df[[MappingForamt.Ref_id]].drop_duplicates().merge(ref_df_copy, how='left')        mapped_references.to_csv(self.paths.reference, index=False)        mapping_df.to_csv(self.paths.mapping, index=False)    @time_it    def merge_similar_mapped_reference(self, mapping_df, references):        mapped_references = mapping_df[[MappingForamt.Ref_id, MappingForamt.Region]].merge(references,                                                                                           on=[MappingForamt.Ref_id,                                                                                               MappingForamt.Region],                                                                                           how='left')        ref_count_before = len(mapped_references[MappingForamt.Ref_id].drop_duplicates())        mapped_references[ReferenceFormat.Original_Id] = mapped_references[CurrentStateFormat.Reference_id]        UNIQUE_SEQUENCE = 'unique_sequence'        UNIQUE_SEQUENCES_GROUP = 'unique_sequence_group'        UNIQUE_SEQUENCES_GROUP_ID = 'unique_sequences_group_id'        mapped_references[UNIQUE_SEQUENCE] = \            mapped_references.groupby([ReferenceFormat.Region] + Base.all).grouper.group_info[0]        # find for each reference the groups ids of his regions        # example -> ref1reg1 -> groupA, ref1reg2 -> groupB --> ref1 --> groupA, groupB        ref_to_unique_groups = pd.DataFrame(            {UNIQUE_SEQUENCES_GROUP: mapped_references.groupby(ReferenceFormat.Original_Id)[                UNIQUE_SEQUENCE].apply(list)}).reset_index()        ref_to_unique_groups[UNIQUE_SEQUENCES_GROUP] = ref_to_unique_groups[UNIQUE_SEQUENCES_GROUP].astype(str)        # find unique references (should be the same in all regions        ref_to_unique_groups[UNIQUE_SEQUENCES_GROUP_ID] = \            ref_to_unique_groups.groupby(UNIQUE_SEQUENCES_GROUP).grouper.group_info[0]        ref_to_unique_groups = ref_to_unique_groups[[UNIQUE_SEQUENCES_GROUP_ID, ReferenceFormat.Original_Id]]        if self.iteration_index == 0:            cols = CurrentStateFormat.Bases.all + [ReferenceFormat.Region, ReferenceFormat.Original_Id]        else:            cols = CurrentStateFormat.Bases.all + [ReferenceFormat.Region, ReferenceFormat.Original_Id,                                                   CurrentStateFormat.Weight]        new_refs = mapped_references.merge(ref_to_unique_groups,                                           on=ReferenceFormat.Original_Id,                                           how='right')        # map all the original ids to unique one, original a -> unique 1, original b -> unique 1        original_ref_id_to_unique_id = new_refs[[ReferenceFormat.Original_Id, UNIQUE_SEQUENCES_GROUP_ID]]        new_refs = new_refs.drop_duplicates([UNIQUE_SEQUENCES_GROUP_ID, ReferenceFormat.Region])        ref_id_to_unique_id = new_refs[[ReferenceFormat.Original_Id, UNIQUE_SEQUENCES_GROUP_ID]]        new_refs = new_refs[cols]        new_refs.rename(columns={ReferenceFormat.Original_Id: ReferenceFormat.Ref_Id}, inplace=True)        new_refs_ids = new_refs[[ReferenceFormat.Ref_Id]].drop_duplicates()        new_refs = new_refs_ids.merge(references, on=ReferenceFormat.Ref_Id, how='left')        ref_count_after = len(new_refs[MappingForamt.Ref_id].drop_duplicates())        logging.info("UNIQUE REFERENCES: before = {}, after = {}".format(ref_count_before, ref_count_after))        new_refs.to_csv(self.paths.reference, index=False)        # only the unique ids will appear in this df: original a -> unique 1        ref_id_to_unique_id.rename(columns={ReferenceFormat.Original_Id: ReferenceFormat.Ref_Id}, inplace=True)        ref_id_to_original_id = ref_id_to_unique_id.merge(original_ref_id_to_unique_id, on=UNIQUE_SEQUENCES_GROUP_ID)        # all the original will be in this list: original 1 -> original 1, original 2 -> original 1        ref_id_to_original_id = ref_id_to_original_id[[ReferenceFormat.Original_Id, ReferenceFormat.Ref_Id]]        ref_id_to_original_id.drop_duplicates(inplace=True)        mapping_df.rename(columns={ReferenceFormat.Ref_Id: ReferenceFormat.Original_Id}, inplace=True)        mapping_df = mapping_df.merge(ref_id_to_original_id, on=ReferenceFormat.Original_Id, how='left')        mapping_df.drop_duplicates([MappingForamt.Region, MappingForamt.Ref_id, MappingForamt.Group_id], inplace=True)        return mapping_df[MappingForamt.full_header]    @time_it    def prepare_references(self, fasta_path):        """        Change the reference format from fasta files to csv format        fasta_path: string                    path to the directory contains fasta file for each region        """        os.chdir(fasta_path)        df = pd.DataFrame(columns=ReferenceFormat.temp_all)        df.to_csv(self.paths.full_reference, index=False, header=True)        files = glob.glob("*.fasta")        for fasta_file in files:            region_ix = self.extract_region_from_fasta_name(fasta_file, len(files))            ref_df = self.process_reference_in_region(fasta_file, self.read_len, region_ix)            ref_df.to_csv(self.paths.full_reference, index=False, header=False, mode='a')    @time_it    def get_unique_amplified_references(self):        """        Find the amplified references using the primers found from the reads.        1. Filter sequences not amplified in the current sample        2. Merge relevant sequences into same reference        3. Save the index from the original reference id to the unique reference id        Work only on the first iteration        """        reference_df = pd.read_csv(self.paths.full_reference, index_col=False)        # Filter sequences not amplified in current sample        # Extract the primer from each reference sequence        for base in Base.all:            # get only the primers bits            logging.info("Base: {}, primer size = {}".format(base, self.primers.min_size))            reference_df[base + '_primer'] = reference_df[base].apply(                lambda r: int(r) >> (2 * self.read_len - self.primers.min_size))        # Calculate the similarity score between each reference and the primers in the relevant region        # Filter the references with low score        reference_by_region = reference_df.groupby(HeadersFormat.Region)        amplified_references_list = []        for region, ref_df_in_region in reference_by_region:            logging.info("region = {}".format(region))            ref_df = ref_df_in_region.reset_index().copy()            ref_df['Score'] = 0            primers = self.primers.all[region]            for primer in primers:                logging.info("primer = {}".format(primer))                bin_primer = sequence_to_bin(primer[:self.primers.min_size])                a = np.bitwise_and(ref_df[Base.A + '_primer'], int(bin_primer[0]))                c = np.bitwise_and(ref_df[Base.C + '_primer'], int(bin_primer[1]))                g = np.bitwise_and(ref_df[Base.G + '_primer'], int(bin_primer[2]))                t = np.bitwise_and(ref_df[Base.T + '_primer'], int(bin_primer[3]))                score_as_bin = np.bitwise_or(np.bitwise_or(a, c), np.bitwise_or(g, t))                logging.info("primer = {}".format(primer))                # calculate the score as the percents of the primers match                ref_df['curr_score'] = 0                for iBit in range(0, self.primers.min_size + 8, 8):                    ref_df['Score_8lsb'] = score_as_bin % (2 ** 8)                    ref_df['curr_score'] = ref_df['curr_score'] + np.unpackbits([ref_df['Score_8lsb'].astype('uint8')],                                                                                axis=0).sum(axis=0) / float(                        self.primers.min_size)                    score_as_bin = np.right_shift(score_as_bin, 8)                ref_df['Score'] = ref_df[['Score', 'curr_score']].max(axis=1)                logging.info(                    "min score = {}, max score = {}, length = {}".format(ref_df['Score'].min(), ref_df['Score'].max(),                                                                         len(ref_df['Score'])))            amplified_references_in_region = ref_df[ref_df['Score'] >= self.primers.similarity_score_th]            amplified_references_list.append(amplified_references_in_region)            logging.info("region = {}, len ref_df above th: {}".format(region, len(amplified_references_in_region)))        amplified_references = pd.concat(amplified_references_list, ignore_index=True)        logging.info(            "amplified references = {}, full reference = {}".format(len(amplified_references), len(reference_df)))        # Remove duplications        regions = amplified_references[HeadersFormat.Region].unique()        ref_by_region_list = []        columns_for_similarity_test = []        for region in regions:            reference_by_region = amplified_references[amplified_references[HeadersFormat.Region] == region]            rename_dict = {}            for base in Base.all:                rename_dict[base] = "Region{}_base{}".format(region, base)                columns_for_similarity_test.append("Region{}_base{}".format(region, base))            reference_by_region = reference_by_region[[ReferenceFormat.Original_Id] + Base.all].rename(                columns=rename_dict)            ref_by_region_list.append(reference_by_region)        amplified_references = ref_by_region_list[0]        for i in range(1, len(ref_by_region_list)):            amplified_references = amplified_references.merge(ref_by_region_list[i], on=ReferenceFormat.Original_Id)            amplified_references = amplified_references.fillna(0)        amplified_references = amplified_references.drop_duplicates()        # create index from the original reference id to the unique reference id        amplified_references[ReferenceFormat.Ref_Id] = amplified_references.groupby(            columns_for_similarity_test).grouper.label_info        unique_references_index = amplified_references[            [ReferenceFormat.Ref_Id, ReferenceFormat.Original_Id]].drop_duplicates()        unique_references_index.to_csv(self.paths.unique_ref_to_ref, index=False)        unique_references = unique_references_index.merge(            reference_df[ReferenceFormat.Bases.all + [ReferenceFormat.Region, ReferenceFormat.Original_Id]],            on=ReferenceFormat.Original_Id)        unique_references = unique_references[ReferenceFormat.header].drop_duplicates()        unique_references.to_csv(self.paths.reference, index=False)        logging.info("# unique reference = {} /  # references in database = {}".                     format(len(unique_references.drop_duplicates(ReferenceFormat.Ref_Id)),                            len(reference_df.drop_duplicates(ReferenceFormat.Original_Id))))    @time_it    def prepare_reads(self, fastq_path, reversed_fastq_path, read_len, unique_group_size_fraction=100000):        logging.info("Start processing: fastq path = %s, reverse reads fastq path = %s", fastq_path,                     reversed_fastq_path)        base_comp_dict = {Base.A: Base.T,                          Base.G: Base.C,                          Base.C: Base.G,                          Base.T: Base.A,                          Base.N: Base.N}        data_dicts = []        read_index = 0        all_reads_counter = 0        short_reads = 0        for (record, reversed_record) in izip(SeqIO.parse(fastq_path, "fastq"),                                              SeqIO.parse(reversed_fastq_path, "fastq")):            read = record.seq.__str__()[:read_len]            reversed_read = reversed_record.seq.__str__()[:read_len][::-1]            if len(read + reversed_read) == 2 * read_len:                read_quals = record.letter_annotations['phred_quality'][:read_len]                reversed_read_quals = reversed_record.letter_annotations['phred_quality'][:read_len][::-1]            else:                short_reads += 1                all_reads_counter += 1                continue            quality = read_quals + reversed_read_quals            second_read = ''.join(map(lambda base: base_comp_dict[base], reversed_read))            bin_seq = sequence_to_bin(read + second_read)            region = self.primers.get_region_for_read(read)            if (len(read + second_read) != 2 * read_len) or (len(quality) != 2 * read_len):                raise Exception("Read length {}, {}!!!".format(len(read + second_read), len(quality)))            if region is not None:                record_dict = self.reads_full_data_format.get_dict(read_index, region, bin_seq, quality)                data_dicts.append(record_dict)                read_index += 1            all_reads_counter += 1        logging.info(            "Reads with matched region found = {} out of {} reads, short_reads={}".format(read_index, all_reads_counter,                                                                                          short_reads))        reads_df = pd.DataFrame(data_dicts, columns=self.reads_full_data_format.all)        reads_df[ReadsFullDataFormat.Quals] = reads_df[ReadsFullDataFormat.Quals].astype(str)        if self.debug_mode:            reads_df.to_csv(self.paths.read_quals, columns=self.reads_full_data_format.all, index=False)        reads_df_grouped = reads_df.groupby(            [self.reads_full_data_format.Region] + self.reads_full_data_format.Bases.all)        reads_df[ReadsFormat.Group_id] = reads_df_grouped.grouper.label_info        reads_df[MappingForamt.Count] = reads_df_grouped[ReadsFormat.Id].transform(len)        reads_full_size = len(reads_df)        unique_group_minimum_size = reads_full_size / unique_group_size_fraction        reads_df = reads_df[reads_df[MappingForamt.Count] > unique_group_minimum_size]        reads_df[[ReadsFormat.Id, ReadsFormat.Group_id, ReadsFormat.Quals]].to_csv(self.paths.read_quals, index=False)        unique_reads_df = reads_df[[MappingForamt.Count, self.reads_full_data_format.Region,                                    self.reads_full_data_format.Group_id] + self.reads_full_data_format.Bases.all].drop_duplicates()        self.n_reads = len(reads_df)        n_reads_groups = len(unique_reads_df)        logging.info("Dropped {}/{} reads (too low frequency). # of reads groups = {}".                     format(reads_full_size - self.n_reads, reads_full_size, n_reads_groups))        return unique_reads_df    @time_it    def calc_prob_n(self):        """            Pr(N=n) -> Pr(S[i] = N) for each base in each sequence, for each N {A, C, G, T}            The result is 4 list for each sequence. ProbN_A, ProbN_C, ProbN_G, ProbN_T            Saved into the CurrentStateTable.            If read or sequence is new this round (not seen at t-1), then            there is no Pr(S|R) from previous round,            so we 1 instead. (same probability for each Pr(S|R)           Initial iteration: all reads and seqs are new, so all calcs            of Pr(N=n) use the Pr(S|R) as weighting factor instead of            previous round's posterior.        """        # Get data:        bases = [Base.A, Base.C, Base.G, Base.T]        current_state_df, mapping_df, reads_df, posteriors_df = self.get_dfs_for_calc_prob_n(bases)        logging.info("length of dataframes: reads = {}, mapping = {}, current_state_df = {}"                     .format(len(reads_df), len(mapping_df), len(current_state_df)))        reads_data_columns, ref_data_columns, prob_success_data_columns, prob_failure_data_columns, rename_dict, prev_prob_n_columns = self.get_columns_for_calc_prob_n(            bases)        if self.iteration_index > 0:            prev_prob_n_full_dict = {Base.A: pd.read_csv(self.paths.prev_prob_n[Base.A], index_col=False),                                     Base.C: pd.read_csv(self.paths.prev_prob_n[Base.C], index_col=False),                                     Base.G: pd.read_csv(self.paths.prev_prob_n[Base.G], index_col=False),                                     Base.T: pd.read_csv(self.paths.prev_prob_n[Base.T], index_col=False)}        else:            prev_prob_n_full_dict = {}            for base in bases:                prob_n_cols = [HeadersFormat.Region, MappingForamt.Ref_id] + prev_prob_n_columns[base]                prev_prob_n_full_dict[base] = pd.DataFrame(columns=prob_n_cols)        mapping_grouped_by_ref = mapping_df.groupby(CurrentStateFormat.Reference_id)        logging.info("calc_prob_n: amount of reference groups: {}".format(len(mapping_grouped_by_ref)))        prob_n_full_dict = parallelize_calc_prob_n_for_single_ref(mapping_grouped_by_ref,                                                                  current_state_df,                                                                  reads_df,                                                                  posteriors_df,                                                                  prev_prob_n_full_dict,                                                                  self.iteration_index,                                                                  prob_success_data_columns,                                                                  prob_failure_data_columns,                                                                  prev_prob_n_columns,                                                                  reads_data_columns,                                                                  rename_dict)        logging.info("Done parallelize_calc_prob_n_for_single_ref")        for base in bases:            base_rename_dict = {}            for i in range(2 * self.read_len):                base_rename_dict[str(i)] = str(i) + base + "_prob_n"            prob_n_full_dict[base] = pd.concat(prob_n_full_dict[base], ignore_index=True)            prob_n_for_next_itr = prob_n_full_dict[base].rename(columns=base_rename_dict)            prob_n_for_next_itr.to_csv(self.paths.prob_n[base], index=False)        return prob_n_full_dict    def get_dfs_for_calc_prob_n(self, bases):        """        get the relevant dataframe for the calculation of prob N.        :param bases:        :return: current_state_df, mapping_df, reads_df, posteriors_df        """        reads_df = pd.read_csv(self.paths.read_quals, index_col=False)        reads_df[ReadsFormat.Quals] = reads_df[ReadsFormat.Quals].apply(literal_eval)        mapping_df = pd.read_csv(self.paths.mapping, index_col=False)        current_state_df = pd.read_csv(self.paths.current_state, index_col=False)        posteriors_df = None        if self.iteration_index > 0:            posteriors_df = pd.read_csv(self.paths.posteriors, index_col=False)        for base in bases:            mapping_df[base] = mapping_df[base].apply(lambda r: bin(int(r))[2:].zfill(2 * self.read_len))            current_state_df[base] = current_state_df[base].apply(lambda r: bin(int(r))[2:].zfill(2 * self.read_len))            current_state_df[CurrentStateFormat.ProbN + base] = 0        for i in range(2 * self.read_len):            reads_df[str(i) + "_prob_success"] = reads_df[ReadsFormat.Quals].apply(                lambda r: quals.ONE_MINUS_P[int(r[i])])            reads_df[str(i) + "_prob_fail"] = reads_df[ReadsFormat.Quals].apply(lambda r: quals.P_DIV_3[int(r[i])])            for base in Base.all:                mapping_df[str(i) + base] = mapping_df[base].apply(lambda r: int(r[i]))                current_state_df[str(i) + base] = current_state_df[base].apply(lambda r: int(r[i]))        return current_state_df, mapping_df, reads_df, posteriors_df    @time_it    def get_dfs_for_calc_likelihoods(self, bases):        reads_df = pd.read_csv(self.paths.read_quals, index_col=False)        reads_df[ReadsFormat.Quals] = reads_df[ReadsFormat.Quals].apply(literal_eval)        mapping_df = pd.read_csv(self.paths.mapping, index_col=False)        # current_state_df = pd.read_csv(self.paths.current_state, index_col=False)        posteriors_df = None        if self.iteration_index > 0:            posteriors_df = pd.read_csv(self.paths.posteriors, index_col=False)        for base in bases:            mapping_df[base] = mapping_df[base].apply(int).apply(lambda r: bin(r)[2:].zfill(2 * self.read_len))            # current_state_df[base] = current_state_df[base].apply(int).apply(            #     lambda r: bin(r)[2:].zfill(2 * self.read_len))        for i in range(2 * self.read_len):            reads_df[str(i) + "_prob_success"] = reads_df[ReadsFormat.Quals].apply(                lambda r: quals.ONE_MINUS_P[int(r[i])])            reads_df[str(i) + "_prob_fail"] = reads_df[ReadsFormat.Quals].apply(lambda r: quals.P_DIV_3[int(r[i])])            for base in Base.all:                mapping_df[str(i) + base] = mapping_df[base].apply(lambda r: int(r[i]))                # current_state_df[str(i) + base] = current_state_df[base].apply(lambda r: int(r[i]))        return mapping_df, reads_df, posteriors_df    def get_columns_for_calc_prob_n(self, bases):        reads_data_columns = {Base.A: [], Base.C: [], Base.G: [], Base.T: []}        ref_data_columns = {Base.A: [], Base.C: [], Base.G: [], Base.T: []}        prev_prob_n_columns = {Base.A: [], Base.C: [], Base.G: [], Base.T: []}        prob_success_data_columns = []        prob_failure_data_columns = []        rename_dict = {}        for i in range(2 * self.read_len):            for base in bases:                reads_data_columns[base] += [str(i) + base + "_read"]                ref_data_columns[base] += [str(i) + base + "_ref"]                prev_prob_n_columns[base] += [str(i) + base + "_prob_n"]                rename_dict.update({str(i) + base + "_read": str(i), str(i) + base + "_ref": str(i),                                    str(i) + base + "_prob_n": str(i)})            prob_success_data_columns += [str(i) + "_prob_success"]            prob_failure_data_columns += [str(i) + "_prob_fail"]            rename_dict.update({str(i) + "_prob_success": str(i), str(i) + "_prob_fail": str(i)})        return reads_data_columns, ref_data_columns, prob_success_data_columns, prob_failure_data_columns, rename_dict, prev_prob_n_columns    def get_columns_for_calc_likelihoods(self, bases):        reads_data_columns = {Base.A: [], Base.C: [], Base.G: [], Base.T: []}        # ref_data_columns = {Base.A: [], Base.C: [], Base.G: [], Base.T: []}        prob_success_data_columns = []        prob_failure_data_columns = []        rename_dict = {}        prob_n_columns = []        for i in range(2 * self.read_len):            for base in bases:                reads_data_columns[base].append(str(i) + base)                # ref_data_columns[base] += [str(i) + base + "_ref"]                rename_dict.update({str(i) + base: str(i)})            prob_success_data_columns.append(str(i) + "_prob_success")            prob_failure_data_columns.append(str(i) + "_prob_fail")            prob_n_columns.append(str(i))            rename_dict.update({str(i) + "_prob_success": str(i), str(i) + "_prob_fail": str(i)})        return prob_n_columns, reads_data_columns, prob_success_data_columns, prob_failure_data_columns, rename_dict    @time_it    def calc_likelihoods(self, prob_n_dict):        """        Calc the P(r|S)        this is the first part of the calculation of P(S|r)        P(r|S) = Mul_k(sum_n(P(b_k|n)P(n)))        e.g:            exp ( sum over all bases in each read of                    log(                        sum over {A, C, G, T} of P(S[x] == N)*P(r[x] == N) were S is the reference which the read mapped to.                        )                )        e^(log_a + log_b) = e^(log_a)*e^(log_b) = a*b (faster way)        :return:        """        bases = [Base.A, Base.C, Base.G, Base.T]        mapping_df, reads_df, posteriors_df = self.get_dfs_for_calc_likelihoods(bases)        prob_n_columns, reads_data_columns, prob_success_data_columns, prob_failure_data_columns, rename_dict = self.get_columns_for_calc_likelihoods(            bases)        mapping_df_size = mapping_df.memory_usage(index=True, deep=True).sum()        row_size_mapping_df = mapping_df_size / len(mapping_df)        # all prob N dataframe has the same row size        probN_size = prob_n_dict['A'].memory_usage(index=True, deep=True).sum()        row_size_probN = probN_size / len(prob_n_dict['A'])        reads_df_size = reads_df.memory_usage(index=True, deep=True).sum()        row_size_reads_df = reads_df_size / len(reads_df)        merged_row_size = row_size_reads_df + row_size_mapping_df + row_size_probN        merge_len = len(reads_df[[HeadersFormat.Group_id]].merge(mapping_df[[HeadersFormat.Group_id]], how='left'))        merge_size = mapping_df_size + 4 * probN_size + merge_len * merged_row_size        free_memory = psutil.virtual_memory().free        nChunks = 10 * int(merge_size / free_memory + 1)        logging.info("LIKELIHOOD: number of chunks = {}, mapping size = {}, reads size = {}, merge length = {}".                     format(nChunks, len(mapping_df), len(reads_df), merge_len))        likelihoods = []        chunk_size = 1000        chunks = int(len(reads_df) / chunk_size) + 1        logging.info(            "OLD CHUNKS LIKELIHOOD: chunk size = {}, number of chunks = {}, mapping size = {}, reads size = {}".format(                chunk_size, chunks, len(mapping_df), len(reads_df)))        for chunk in np.array_split(reads_df, nChunks):            chunk_reads_full_data = pd.DataFrame.merge(chunk, mapping_df, on=HeadersFormat.Group_id, how='left')            res_dict = {Base.A: [], Base.C: [], Base.G: [], Base.T: []}            for base in bases:                chunk_full_data_df = chunk_reads_full_data.merge(prob_n_dict[base],                                                                 on=[MappingForamt.Ref_id, HeadersFormat.Region],                                                                 how='left')                reads = chunk_full_data_df[reads_data_columns[base]]                reads.rename(columns=rename_dict, inplace=True)                prob_n = chunk_full_data_df[prob_n_columns]                prob_success = chunk_full_data_df[prob_success_data_columns].rename(columns=rename_dict)                prob_fail = chunk_full_data_df[prob_failure_data_columns].rename(columns=rename_dict)                reads_prob = reads.multiply(prob_success) + (1 - reads).multiply(prob_fail)                res_dict[base] = prob_n.multiply(reads_prob)            res = res_dict[Base.A] + res_dict[Base.C] + res_dict[Base.G] + res_dict[Base.T]            res = res[prob_n_columns].applymap(np.log)            likelihood = pd.DataFrame({HeadersFormat.Likelihood: res.apply(sum, axis=1)}).apply(np.exp)            likelihood[MappingForamt.Ref_id] = chunk_reads_full_data[MappingForamt.Ref_id]            likelihood[ReadsFullDataFormat.Id] = chunk_reads_full_data[ReadsFullDataFormat.Id]            likelihoods.append(likelihood)        # logging.info("likelihood, before concat")        likelihood_df = pd.concat(likelihoods, ignore_index=True)        print("return likelihood df, size = {}".format(likelihood_df.memory_usage(index=True, deep=True).sum()))        return likelihood_df    @time_it    def calc_posteriors(self, prob_n_dict):        """        P(S|r) = P(r|S)P(s) / sum_i(P(r|Si)P(Si))        :param prob_n_dict:        :return:        """        likelihood_df = self.calc_likelihoods(prob_n_dict)        priors_df = pd.read_csv(self.paths.current_state, index_col=False)[            [CurrentStateFormat.Reference_id, CurrentStateFormat.Priors]].drop_duplicates()        posteriors = likelihood_df.merge(priors_df, on=HeadersFormat.Unique_Ref_id, how='left')        logging.info(            "posteriors = {} refs".format(len(posteriors.drop_duplicates(HeadersFormat.Unique_Ref_id))))        # P(r|S)P(s)        posteriors[PosteriorsFormat.Posterior] = posteriors[PosteriorsFormat.Likelihood].astype(float) * posteriors[            CurrentStateFormat.Priors].astype(float)        # denominator_df --> sum_i(P(r|Si)P(Si))        denominator_df = pd.DataFrame({'denominator': posteriors.groupby(PosteriorsFormat.Read_id)[            PosteriorsFormat.Posterior].sum()}).reset_index()        posteriors = posteriors.merge(denominator_df, how='left')        # P(S|r) = P(r|S)P(s) / sum_i(P(r|Si)P(Si))        zero_posteriors = posteriors[posteriors['denominator'] == 0]        zero_posteriors[PosteriorsFormat.Posterior] = 0        logging.info(            "zero_posteriors = {} refs".format(len(zero_posteriors.drop_duplicates(HeadersFormat.Unique_Ref_id))))        non_zero_posteriors = posteriors[posteriors['denominator'] != 0]        non_zero_posteriors[PosteriorsFormat.Posterior] = \            non_zero_posteriors[PosteriorsFormat.Posterior] / non_zero_posteriors['denominator']        logging.info(            "non_zero_posteriors = {} refs".format(                len(non_zero_posteriors.drop_duplicates(HeadersFormat.Unique_Ref_id))))        posteriors = pd.concat([zero_posteriors, non_zero_posteriors])        logging.info(            "posteriors = {} refs".format(                len(posteriors.drop_duplicates(HeadersFormat.Unique_Ref_id))))        posteriors = posteriors.fillna(0)        posteriors = posteriors[[PosteriorsFormat.Posterior,                                 PosteriorsFormat.Read_id,                                 PosteriorsFormat.Ref_id]].drop_duplicates()        posteriors.to_csv(self.paths.posteriors, index=False)    @time_it    def calc_priors(self):        """            calc the priors P(s_i) = sum_j(P(s_i|rj)/ sum_i, j(P(s_i|r_j)            where P(s|r) is the posteriors.        """        posteriors_df = pd.read_csv(self.paths.posteriors, index_col=False)        new_priors = posteriors_df.groupby(PosteriorsFormat.Ref_id)[PosteriorsFormat.Posterior].sum().reset_index()        # normalize in the reference # regions:        curr_state_df = pd.read_csv(self.paths.current_state, index_col=False)        ref_weight = curr_state_df[[CurrentStateFormat.Reference_id, CurrentStateFormat.Weight]].drop_duplicates()        new_priors = ref_weight.merge(new_priors, on=CurrentStateFormat.Reference_id, how='right')        new_priors[CurrentStateFormat.Priors] = new_priors. \            apply(lambda r: r[PosteriorsFormat.Posterior] / r[CurrentStateFormat.Weight], axis=1)        posteriors_sum = new_priors.Priors.sum()        new_priors[CurrentStateFormat.Priors] = new_priors.apply(            lambda r: r[CurrentStateFormat.Priors] / posteriors_sum, axis=1)        new_priors = new_priors[[CurrentStateFormat.Priors, CurrentStateFormat.Reference_id]].drop_duplicates()        # get privius priors values:        old_priors = curr_state_df[[CurrentStateFormat.Reference_id, CurrentStateFormat.Priors]].drop_duplicates()        # Take only only the prior values which where not calculated this round.        old_priors = old_priors[            (~old_priors[CurrentStateFormat.Reference_id].isin(new_priors[PosteriorsFormat.Ref_id]))]        priors_df = pd.concat([old_priors, new_priors], ignore_index=True, sort=True)        # update curr state with the new priors values.        cols = [CurrentStateFormat.Reference_id,                CurrentStateFormat.Region,                CurrentStateFormat.Weight] + CurrentStateFormat.Bases.all        curr_state_df = curr_state_df[cols]        curr_state_df = curr_state_df.merge(priors_df)        curr_state_df = curr_state_df.drop_duplicates()        curr_state_df.to_csv(self.paths.current_state, index=False)    @time_it    def update_references(self, prob_n_dict):        curr_state_df = pd.read_csv(self.paths.current_state, index_col=False)        curr_state_df.to_csv(self.paths.current_state + "update_references.csv", index=False)        full_posteriors = pd.read_csv(self.paths.posteriors, index_col=False)        # create table of the probabilities: the columns contains the the probability of each base in each index - 0A,        # 0C, 0G, 0T, 1A, etc the rows are the references and the regions        prob_n_ac = pd.merge(prob_n_dict[Base.A], prob_n_dict[Base.C],                             on=[HeadersFormat.Unique_Ref_id, HeadersFormat.Region], suffixes=(Base.A, Base.C))        prob_n_gt = pd.merge(prob_n_dict[Base.G], prob_n_dict[Base.T],                             on=[HeadersFormat.Unique_Ref_id, HeadersFormat.Region], suffixes=(Base.G, Base.T))        prob_n_full = pd.merge(prob_n_ac, prob_n_gt, on=[HeadersFormat.Unique_Ref_id, HeadersFormat.Region])        base_ix_columns = []        bases_columns = []        second_bases_columns = []        second_probs_columns = []        for i in range(0, 2 * self.read_len):            base_ix_columns.append(str(i))  # the probability of the most probable base in index i            bases_columns.append('base_' + str(i))  # the most probable base to be in in index i            second_bases_columns.append('2-base_' + str(i))  # the second most probable base to be in in index i            second_probs_columns.append('2-' + str(i))  # the probability of the second most probable base in index i        for base_ix in base_ix_columns:            ix_cols = [base_ix + Base.A, base_ix + Base.C, base_ix + Base.G, base_ix + Base.T]            # prob_n_for_ix: row for each reference-region pair            prob_n_for_ix = prob_n_full[ix_cols]            #  take the second best base, if the probability is larger than the the split threshold            prob_n_full[['2-' + base_ix, base_ix]] = pd.DataFrame(np.sort(prob_n_for_ix.values)[:, -2:])            prob_n_full['2-' + base_ix] = prob_n_full['2-' + base_ix].apply(                lambda r: r if r > self.th.min_minor_prob_for_split else 0)            arank = prob_n_for_ix.apply(np.argsort, axis=1)            argsort_dict = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}            # prob_n_full[['2-base_' + base_ix, 'base_' + base_ix]] = arank[[2, 3]].applymap(lambda x: argsort_dict[x])            try:                prob_n_full[['2-base_' + base_ix, 'base_' + base_ix]] = arank.iloc[:, [2, 3]].applymap(                    lambda x: argsort_dict[x])            except Exception as ex:                logging.exception("Exception: {}".format(str(ex)))                logging.error("base ix = {}".format(base_ix))                for index, row in arank.iloc[:, [2, 3]].iterrows():                    if row[0] not in range(4) or row[1] not in range(4):                        logging.error(arank.iloc[index])                        logging.error(prob_n_for_ix.iloc[index])                raise ex            # select best probability if the second is too low.            prob_n_full['2-base_' + base_ix] = prob_n_full.apply(                lambda r: r['base_' + base_ix] if r['2-' + base_ix] == 0 else r['2-base_' + base_ix], axis=1)        prob_n_full['best_sequence'] = prob_n_full[bases_columns].apply(lambda r: "".join(r.tolist()), axis=1)        prob_n_full['2-best_sequence'] = prob_n_full[second_bases_columns].apply(lambda r: "".join(r.tolist()), axis=1)        prob_n_full['second_best_counter'] = prob_n_full[second_probs_columns].apply(np.count_nonzero, axis=1)        prob_n_full[second_probs_columns] = prob_n_full[second_probs_columns].replace(0, np.NaN)        prob_n_full['minor'] = prob_n_full[second_probs_columns].mean(axis=1)        prob_n_full['minor'].fillna(0, inplace=True)        prob_n_full[second_probs_columns].fillna(0, inplace=True)        prob_n_relevant = prob_n_full[            [CurrentStateFormat.Reference_id, CurrentStateFormat.Region, 'best_sequence', '2-best_sequence',             'second_best_counter', 'minor']]        max_prob_n_full_data = prob_n_relevant.merge(curr_state_df,                                                     on=[HeadersFormat.Unique_Ref_id, HeadersFormat.Region])        logging.info(            "NAN: minor = {}, Prior = {}, Weight = {}".format(max_prob_n_full_data['minor'].isnull().values.sum(),                                                              max_prob_n_full_data[                                                                  CurrentStateFormat.Priors].isnull().values.sum(),                                                              max_prob_n_full_data[                                                                  CurrentStateFormat.Weight].isnull().values.sum()))        max_prob_n_full_data['expected_coverage_minor'] = max_prob_n_full_data['minor'] * max_prob_n_full_data[            CurrentStateFormat.Priors] * self.n_regions / max_prob_n_full_data[CurrentStateFormat.Weight]        if self.allow_split and self.iteration_index > 0:            # 1. The rate of changed bases in the split reference: (how many bases will change)/(length of read)            # 2. The min rate to create a new split reference: (prior*minor) * (# of regions/ reference weight)            max_prob_n_full_data['is_split'] = (max_prob_n_full_data['second_best_counter'] / float(                2 * self.read_len) < self.th.max_changed_bases_rate_for_split) \                                               & (max_prob_n_full_data[                                                      'expected_coverage_minor'] >= self.th.min_coverage_for_split)            max_prob_n_full_data['is_split'].update(                max_prob_n_full_data.groupby(CurrentStateFormat.Reference_id)['is_split'].transform(any).reset_index()[                    'is_split'])            max_prob_n_full_data['avg_minor'] = \                max_prob_n_full_data.groupby(CurrentStateFormat.Reference_id)['minor'].transform(np.mean).reset_index()[                    'minor']        else:            max_prob_n_full_data['is_split'] = False            max_prob_n_full_data['avg_minor'] = 0        split_reference_suffix = self.iteration_index * math.pow(10, -1 * self.iteration_index - 2)        new_refs = _get_new_references(max_prob_n_full_data, split_reference_suffix)        new_posteriors = _get_new_posterior(max_prob_n_full_data, full_posteriors,                                            split_reference_suffix).drop_duplicates()        new_refs.to_csv(self.paths.current_state, index=False)        new_posteriors.to_csv(self.paths.posteriors, index=False)        logging.info("{}/{} references found".format(len(new_refs.drop_duplicates(ReferenceFormat.Ref_Id)), len(            curr_state_df.drop_duplicates(CurrentStateFormat.Reference_id))))        return new_refs    @time_it    def is_stable_state(self, new_reference_df, is_last_iteration):        if self.iteration_index <= 0:            return False        new_df = new_reference_df[[CurrentStateFormat.Reference_id, CurrentStateFormat.Priors]].drop_duplicates()        merged = self.prev_priors_for_stability_test.merge(new_df, on=CurrentStateFormat.Reference_id, how='outer')        merged.fillna(0, inplace=True)        merged['Prior_diff'] = merged[CurrentStateFormat.Priors + "_x"] - merged[CurrentStateFormat.Priors + "_y"]        merged['is_stable'] = merged['Prior_diff'].apply(lambda r:                                                         True if abs(                                                             r) < self.th.max_priors_diff_for_stability_test else False)        if False in merged['is_stable'].tolist() and not is_last_iteration:            logging.info("Priors are not stable")            logging.info("Diff: {}".format(merged[merged['is_stable'] == False]))            return False        else:            logging.info("Stable state!")            new_reference_df['Sequence'] = new_reference_df.apply(                lambda r: bin_to_sequence(r[CurrentStateFormat.Bases.A],                                          r[CurrentStateFormat.Bases.C],                                          r[CurrentStateFormat.Bases.G],                                          r[CurrentStateFormat.Bases.T],                                          2 * self.read_len), axis=1)            new_reference_df[[CurrentStateFormat.Reference_id,                              CurrentStateFormat.Region,                              CurrentStateFormat.Priors,                              'Sequence']].to_csv(self.paths.final_results, index=False)            self.do_post_process()            return True    @time_it    def do_iteration(self, is_last_iteration):        prob_n_dict = self.calc_prob_n()        self.calc_posteriors(prob_n_dict)        self.calc_priors()        new_reference_df = self.update_references(prob_n_dict)        is_stable = self.is_stable_state(new_reference_df, is_last_iteration)        return is_stable    @time_it    def do_post_process(self):        """        For each sequence in the result find the best match in the DB        1. find the best match in the original bit format        2. find id's match in the dict file.        :return:        """        results_df = pd.read_csv(self.paths.final_results, index_col=False)        results_df['changed_reference_id'] = results_df[CurrentStateFormat.Reference_id]        results_df[CurrentStateFormat.Reference_id] = results_df[CurrentStateFormat.Reference_id].apply(            lambda i: int(i))        logging.info("try to read csv: {}".format(self.paths.unique_ref_to_ref))        if os.path.exists(self.paths.unique_ref_to_ref):            logging.info("try to read csv: {}".format(self.paths.unique_ref_to_ref))        else:            logging.info("try to read csv: {} - file not exists".format(self.paths.unique_ref_to_ref))        ids_dict_df = pd.read_csv(self.paths.unique_ref_to_ref, index_col=False)        full_result = results_df.merge(ids_dict_df, on=CurrentStateFormat.Reference_id)        logging.info("Headers: ids_dict_df = {}, full_result={}".format(ids_dict_df.columns, full_result.columns))        full_result = full_result[[CurrentStateFormat.Reference_id,                                   ReferenceFormat.Original_Id,                                   'changed_reference_id',                                   CurrentStateFormat.Region,                                   CurrentStateFormat.Priors,                                   'Sequence']]        full_result.to_csv(self.paths.final_results, index=False)def calc_prob_n_for_single_ref(ref_group_id,                               mapping_ref_df,                               current_state_df,                               reads_df,                               posteriors_df,                               prev_prob_n_full_dict,                               iteration_ix,                               prob_success_data_columns,                               prob_failure_data_columns,                               prev_prob_n_columns,                               reads_data_columns,                               rename_dict                               ):    mapping_ref_df = mapping_ref_df.reset_index()    ref_full_data = mapping_ref_df.merge(current_state_df,                                         on=[CurrentStateFormat.Reference_id, CurrentStateFormat.Region], how='left',                                         suffixes=('_read', '_ref'))    ref_full_data = ref_full_data.merge(reads_df, on=MappingForamt.Group_id, how='left')    if posteriors_df is not None:        ref_full_data = ref_full_data.merge(posteriors_df, on=[ReadsFullDataFormat.Id, CurrentStateFormat.Reference_id],                                            how='left')    memory_usage = ref_full_data.memory_usage(index=True, deep=True).sum() / (10 ** 9)    prob_n_for_base = {}    prob_success = ref_full_data[prob_success_data_columns].copy()    prob_success.rename(columns=rename_dict, inplace=True)    prob_fail = ref_full_data[prob_failure_data_columns].copy()    prob_fail.rename(columns=rename_dict, inplace=True)    map_weight = ref_full_data[MappingForamt.Map_weight]    if iteration_ix == 0:        posteriors = map_weight    else:        ref_full_data[HeadersFormat.Posterior] = ref_full_data[HeadersFormat.Posterior].fillna(            ref_full_data[MappingForamt.Map_weight])        posteriors = ref_full_data[HeadersFormat.Posterior]    for base in Base.all:        ref_full_data = ref_full_data.merge(prev_prob_n_full_dict[base],                                            on=[CurrentStateFormat.Reference_id, CurrentStateFormat.Region],                                            how='left')        ref_full_data.fillna(value=1, inplace=True)        prev_prob_n = ref_full_data[prev_prob_n_columns[base]].rename(columns=rename_dict)        ref_full_data = ref_full_data.drop(columns=prev_prob_n_columns[base])        reads = ref_full_data[reads_data_columns[base]].rename(columns=rename_dict)        ref_full_data = ref_full_data.drop(columns=reads_data_columns[base])        # calculate Pr(n_jk|b_ik)Pr(n_jk) where j in bacteria, i is read and k is the base index:        prob_n_for_base.update(            {base: (reads.multiply(prob_success) + (1 - reads).multiply(prob_fail)).multiply(prev_prob_n)})    # calculate Pr(b_ik): sum_{A, C, G, T}[Pr(n_jk|b_ik)Pr(n_jk)] (for each k, i, j)    prob_read_bases = prob_n_for_base[Base.A] + prob_n_for_base[Base.C] + prob_n_for_base[Base.G] + prob_n_for_base[        Base.T]    for base in Base.all:        # for each base we calculate P(nk = base):        if prob_n_for_base[base].isna().values.any():            logging.info("test")            logging.warn("None in probN {} {} {}".format(base, ref_group_id, prob_n_for_base[base].values))        # calculate Pr(b_ik|n_jk)        prob_n_for_base[base] = prob_n_for_base[base] / prob_read_bases        prob_n_for_base[base] = prob_n_for_base[base].multiply(posteriors, axis='index')        # Calculate P(Nk)for each region of the current reference:        prob_n_for_base[base][HeadersFormat.Region] = ref_full_data[HeadersFormat.Region]        prob_n_for_base[base][HeadersFormat.Unique_Ref_id] = ref_full_data[HeadersFormat.Unique_Ref_id]        prob_n_for_base[base][HeadersFormat.Posterior] = posteriors        prob_n_for_base[base] = prob_n_for_base[base].groupby([HeadersFormat.Region, HeadersFormat.Unique_Ref_id]).sum()        # Divide each sequence (reference + region) in the sum of the posteriors in the specific sequence.        prob_n_for_base[base] = prob_n_for_base[base].divide(prob_n_for_base[base][HeadersFormat.Posterior], axis=0)        # get back the region and the reference id into the data        prob_n_for_base[base].reset_index(inplace=True)        if prob_n_for_base[base].isna().values.any():            logging.warn(                "After computation: None in probN {} {} {}".format(base, ref_group_id, prob_n_for_base[base].values))    logging.info("ref_group_id = {}, Done".format(ref_group_id))    return prob_n_for_basedef parallelize_calc_prob_n_for_single_ref(mapping_grouped_by_ref,                                           current_state_df,                                           reads_df,                                           posteriors_df,                                           prev_prob_n_full_dict,                                           iteration_index,                                           prob_success_data_columns,                                           prob_failure_data_columns,                                           prev_prob_n_columns,                                           reads_data_columns,                                           rename_dict):    mapping_dfs = []    n_refs_in_task = 20    i = 1    single_task = []    for ref_id, map_by_ref in mapping_grouped_by_ref:        if i % n_refs_in_task == 0:            mapping_dfs.append((ref_id, pd.concat(single_task)))            single_task = []        single_task.append(map_by_ref)        i += 1    if len(single_task) > 0:        mapping_dfs.append((-1, pd.concat(single_task)))    list_of_prob_n_for_base_dicts = []    for (group_id, df) in mapping_dfs:        list_of_prob_n_for_base_dicts.append(            calc_prob_n_for_single_ref(group_id, df,                                       current_state_df,                                       reads_df,                                       posteriors_df,                                       prev_prob_n_full_dict,                                       iteration_index,                                       prob_success_data_columns,                                       prob_failure_data_columns,                                       prev_prob_n_columns,                                       reads_data_columns,                                       rename_dict))    full_prob_n_for_base = {'A': [], 'C': [], 'G': [], 'T': []}    for base in Base.all:        for prob_n_for_base in list_of_prob_n_for_base_dicts:            full_prob_n_for_base[base] += [prob_n_for_base[base]]    return full_prob_n_for_basedef _map_single_chunk_parallelize(ref_df, nBits, min_score_for_mapping, reads_df):    # g_id = reads_df[ReadsFormat.Group_id].iloc[0]    reads_and_refs_chunk_df = pd.DataFrame.merge(ref_df, reads_df, on=HeadersFormat.Region, how='right',                                                 suffixes=('_ref', '_read'))    # logging.info("id {}: read len = {}, merge_len = {}".format(g_id, len(reads_df), len(reads_and_refs_chunk_df)))    try:        a = np.bitwise_and(reads_and_refs_chunk_df['A_ref'], reads_and_refs_chunk_df['A_read'])        c = np.bitwise_and(reads_and_refs_chunk_df['C_ref'], reads_and_refs_chunk_df['C_read'])        g = np.bitwise_and(reads_and_refs_chunk_df['G_ref'], reads_and_refs_chunk_df['G_read'])        t = np.bitwise_and(reads_and_refs_chunk_df['T_ref'], reads_and_refs_chunk_df['T_read'])        reads_and_refs_chunk_df['Score_bit'] = np.bitwise_or(np.bitwise_or(a, c), np.bitwise_or(g, t))        reads_and_refs_chunk_df['Score'] = 0        # logging.info("id {} done binary".format(g_id))        for iBit in range(0, nBits + 8, 8):            reads_and_refs_chunk_df['Score_8lsb'] = reads_and_refs_chunk_df['Score_bit'] % (2 ** 8)            reads_and_refs_chunk_df['Score'] = reads_and_refs_chunk_df['Score'] + np.unpackbits(                [reads_and_refs_chunk_df['Score_8lsb'].astype('uint8')], axis=0).sum(axis=0)            reads_and_refs_chunk_df['Score_bit'] = np.right_shift(reads_and_refs_chunk_df['Score_bit'], 8)            # filter to save time            if ((iBit % 16) == 0):                reads_and_refs_chunk_df['tmp_Score'] = reads_and_refs_chunk_df['Score'].copy()                reads_and_refs_chunk_df = reads_and_refs_chunk_df[reads_and_refs_chunk_df['tmp_Score'] >=                                                                  (reads_and_refs_chunk_df.groupby(                                                                      HeadersFormat.Group_id)[                                                                       'tmp_Score'].transform(max) - 5)]                # logging.info("id {} scoring keep {} --> {}".format(g_id, iBit, len(reads_and_refs_chunk_df)))        #        # logging.info("id {} done scoring".format(g_id))        reads_and_refs_chunk_df = reads_and_refs_chunk_df[reads_and_refs_chunk_df['Score'] ==                                                          (reads_and_refs_chunk_df.groupby(HeadersFormat.Group_id)[                                                               'Score'].transform(max))]        reads_and_refs_chunk_df = reads_and_refs_chunk_df[reads_and_refs_chunk_df['Score'] > min_score_for_mapping]        # logging.info("id {}: # Reads ids = {}, # reference mapped = {}".format(g_id, len(reads_df), len(reads_and_refs_chunk_df)))    except Exception, err:        logging.error("read ids = {}".format(reads_df[ReadsFormat.Group_id]))        logging.error("reads_df length = {}".format(len(reads_df)))        logging.error("ref_df length = {}".format(len(ref_df)))        logging.error("nBits = {}, min_score_for_mapping = {}".format(nBits, min_score_for_mapping))        traceback.print_exc()        # raise Exception("read ids = {}".format(reads_df[ReadsFormat.Group_id]))        # # logging.exception("Exception = {}, reads ids = {}".format(str(ex), reads_df[ReadsFormat.Group_id]))        reads_and_refs_chunk_df = pd.DataFrame(columns=reads_and_refs_chunk_df.columns)    return reads_and_refs_chunk_dfdef parallelize_mapping(df_to_split, df, nBits, nChunks, min_score_for_mapping, n_cores=cpu_count()):    df_split = np.array_split(df_to_split, n_cores * nChunks)    pool = Pool(max(1, n_cores - 1))    func = partial(_map_single_chunk_parallelize, df, nBits, min_score_for_mapping)    map_list = pool.map(func, df_split)    logging.info("after pool")    res_df = pd.concat(map_list)    logging.info("after concat")    pool.close()    pool.join()    return res_dfdef get_emirge_iteration_mock():    data_path = "/home/vered/EMIRGE/EMIRGE-data/mock_data/"    reads_fastq_path = data_path + "reads_mock1.fastq"    reversed_reads_fastq_path = data_path + "reads_mock2.fastq"    working_dir = "/home/vered/EMIRGE/EMIRGE-data/mock_tests"    # reference_path = working_dir + "/full_reference_db.csv"    fasta_path = data_path    read_len = 126    emirge_iteration = Smurf2Iteration(working_dir, reads_fastq_path, reversed_reads_fastq_path,                                       fasta_path, read_len)    return emirge_iterationdef offline_post_process(last_results_path, final_results_path, unique_ref_to_ref_path, read_len):    """    For each sequence in the result find the best match in the DB    1. find the best match in the original bit format    2. find id's match in the dict file.    :return:    """    current_state = pd.read_csv(last_results_path, index_col=False)    current_state['Sequence'] = current_state.apply(lambda r: bin_to_sequence(r[CurrentStateFormat.Bases.A],                                                                              r[CurrentStateFormat.Bases.C],                                                                              r[CurrentStateFormat.Bases.G],                                                                              r[CurrentStateFormat.Bases.T],                                                                              2 * read_len), axis=1)    results_df = current_state[[CurrentStateFormat.Reference_id,                                CurrentStateFormat.Region,                                CurrentStateFormat.Priors,                                'Sequence']]    results_df['changed_reference_id'] = results_df[CurrentStateFormat.Reference_id]    results_df[CurrentStateFormat.Reference_id] = results_df[CurrentStateFormat.Reference_id].apply(lambda i: int(i))    logging.info("try to read csv: {}".format(unique_ref_to_ref_path))    if os.path.exists(unique_ref_to_ref_path):        logging.info("try to read csv: {}".format(unique_ref_to_ref_path))    else:        logging.info("try to read csv: {} - file not exists".format(unique_ref_to_ref_path))    ids_dict_df = pd.read_csv(unique_ref_to_ref_path, index_col=False)    full_result = results_df.merge(ids_dict_df, on=CurrentStateFormat.Reference_id)    logging.info("Headers: ids_dict_df = {}, full_result={}".format(ids_dict_df.columns, full_result.columns))    full_result = full_result[[CurrentStateFormat.Reference_id,                               ReferenceFormat.Original_Id,                               'changed_reference_id',                               CurrentStateFormat.Region,                               CurrentStateFormat.Priors,                               'Sequence']]    full_result.to_csv(final_results_path, index=False)def main():    define_logger(logging.INFO)    iteration = get_emirge_iteration_mock()    iteration.do_iteration()if __name__ == '__main__':    main()